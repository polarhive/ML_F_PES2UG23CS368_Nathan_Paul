{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQVL4UhEUn7f"
   },
   "source": [
    "**Objective**: In this lab, you will implement and compare manual grid search with scikit-learn's built-in GridSearchCV for hyperparameter tuning. You'll work with multiple classification algorithms and combine them using voting classifiers.\n",
    "\n",
    "**Learning Goals**:\n",
    "- Understand hyperparameter tuning through grid search\n",
    "- Compare manual implementation with built-in functions\n",
    "- Learn to create and evaluate voting classifiers\n",
    "- Work with multiple real-world datasets\n",
    "- Visualize model performance using ROC curves and confusion matrices\n",
    "\n",
    "**Datasets Used**:\n",
    "1. Wine Quality - Predicting wine quality based on chemical properties\n",
    "2. HR Attrition - Predicting employee turnover\n",
    "3. Banknote Authentication - Detecting counterfeit banknotes\n",
    "4. QSAR Biodegradation - Predicting chemical biodegradability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhTLi8QnUxFG"
   },
   "source": [
    "\n",
    "## Part 1: Import Libraries and Setup\n",
    "\n",
    "First, let's import all the necessary libraries for our machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2OwDmFPCRrMI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas._config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/repos/pesu/ML_F_PES2UG23CS368_Nathan_Paul/Week-4/.venv/lib/python3.13/site-packages/pandas/__init__.py:37\u001b[39m\n\u001b[32m     30\u001b[39m     _module = _err.name\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     32\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not built. If you want to import \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpandas from the source directory, you may need to run \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpython setup.py build_ext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to build the C extensions first.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_err\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     get_option,\n\u001b[32m     39\u001b[39m     set_option,\n\u001b[32m     40\u001b[39m     reset_option,\n\u001b[32m     41\u001b[39m     describe_option,\n\u001b[32m     42\u001b[39m     option_context,\n\u001b[32m     43\u001b[39m     options,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas._config'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, roc_curve,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay, classification_report)\n",
    "# Bypass SSL certificate verification for dataset downloads (optional)\n",
    "try:\n",
    "    import ssl\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0WVjgxFUwIO"
   },
   "source": [
    "# Models and Parameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBkCgnvWRvcK"
   },
   "outputs": [],
   "source": [
    "# The parameter names must match the pipeline step names, e.g., 'classifier__max_depth'\n",
    "# TODO: Define base models (Decision Tree, kNN, Logistic Regression)\n",
    "\n",
    "param_grid_dt = {}\n",
    "\n",
    "param_grid_knn = {}\n",
    "\n",
    "param_grid_lr = {}\n",
    "\n",
    "# ToDo: Create a list of (classifier, param_grid, name) tuples\n",
    "classifiers_to_tune = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Ls-UHJWNU6"
   },
   "source": [
    "## Dataset Loading Functions\n",
    "We'll work with four different datasets to test our algorithms across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUtGII7gWX_a"
   },
   "source": [
    "### 3.1 Wine Quality Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHHtEi5GZjaU"
   },
   "outputs": [],
   "source": [
    "def load_wine_quality():\n",
    "    \"\"\"Load Wine Quality dataset\"\"\"\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "    try:\n",
    "        data = pd.read_csv(url, sep=';')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Wine Quality dataset: {e}\")\n",
    "        return None, None, None, None, \"Wine Quality (Failed)\"\n",
    "\n",
    "    # Create the binary target variable 'good_quality'\n",
    "    data['good_quality'] = (data['quality'] > 5).astype(int)\n",
    "    X = data.drop(['quality', 'good_quality'], axis=1)\n",
    "    y = data['good_quality']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(\"Wine Quality dataset loaded and preprocessed successfully.\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, \"Wine Quality\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COsVTdScWkV6"
   },
   "source": [
    "### 3.2 HR Attrition Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yg6n6BKwZwGU"
   },
   "outputs": [],
   "source": [
    "def load_hr_attrition():\n",
    "    \"\"\"Load IBM HR Attrition dataset\"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(\"data/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"HR Attrition dataset not found. Please place 'WA_Fn-UseC_-HR-Employee-Attrition.csv' inside a 'data/' folder.\")\n",
    "        return None, None, None, None, \"HR Attrition (Failed)\"\n",
    "\n",
    "    # Target: Attrition = Yes (1), No (0)\n",
    "    data['Attrition'] = (data['Attrition'] == 'Yes').astype(int)\n",
    "\n",
    "    # Drop ID-like column\n",
    "    X = data.drop(['EmployeeNumber', 'Attrition'], axis=1, errors='ignore')\n",
    "    y = data['Attrition']\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"IBM HR Attrition dataset loaded and preprocessed successfully.\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, \"HR Attrition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKtpzNthWoUu"
   },
   "source": [
    "### 3.3 Banknote Authentication Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx-CvEtlZ1HO"
   },
   "outputs": [],
   "source": [
    "def load_banknote():\n",
    "    \"\"\"Load Banknote Authentication dataset\"\"\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(url, header=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Banknote dataset: {e}\")\n",
    "        return None, None, None, None, \"Banknote (Failed)\"\n",
    "\n",
    "    # According to UCI: variance, skewness, curtosis, entropy, class (0=authentic, 1=fake)\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Banknote Authentication dataset loaded successfully.\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, \"Banknote Authentication\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvL5YtzRW8QT"
   },
   "source": [
    "\n",
    "### 3.4 QSAR Biodegradation Dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bec4DanzZ60c"
   },
   "outputs": [],
   "source": [
    "def load_qsar_biodegradation():\n",
    "    \"\"\"Load QSAR Biodegradation dataset\"\"\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv\"\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(url, sep=';', header=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading QSAR dataset: {e}\")\n",
    "        return None, None, None, None, \"QSAR (Failed)\"\n",
    "\n",
    "    # Last column is target (RB = ready biodegradable, NRB = not)\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = (data.iloc[:, -1] == 'RB').astype(int)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"QSAR Biodegradation dataset loaded successfully.\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test, \"QSAR Biodegradation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ovvQpiIXN7O"
   },
   "source": [
    "## Part 4: Manual Grid Search Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GJmCWupaGE8"
   },
   "outputs": [],
   "source": [
    "def run_manual_grid_search(X_train, y_train, dataset_name):\n",
    "    \"\"\"Run manual grid search and return best estimators\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING MANUAL GRID SEARCH FOR {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best_estimators = {}\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Adjust parameter grids based on dataset size\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    for classifier_instance, param_grid, name in classifiers_to_tune:\n",
    "        print(f\"--- Manual Grid Search for {name} ---\")\n",
    "        best_score = -1\n",
    "        best_params = None\n",
    "\n",
    "        # TODO: Implement manual grid search for hyperparameter tuning.\n",
    "        # Steps to implement:\n",
    "        # 1. Adjust the feature selection parameter grid to ensure 'k' does not exceed the number of features.\n",
    "        # 2. Generate all combinations of hyperparameters from the adjusted parameter grid.\n",
    "        # 3. For each parameter combination:\n",
    "        #    a. Perform cross-validation (e.g., 5-fold StratifiedKFold).\n",
    "        #    b. For each fold:\n",
    "        #       i. Split the training data into training and validation sets.\n",
    "        #       ii. Build a pipeline with scaling, feature selection, and the classifier.\n",
    "        #       iii. Set the pipeline parameters for the current combination.\n",
    "        #       iv. Fit the pipeline on the training fold.\n",
    "        #       v. Predict probabilities on the validation fold.\n",
    "        #       vi. Compute the AUC score for the fold.\n",
    "        #    c. Compute the mean AUC across all folds for the parameter combination.\n",
    "        #    d. Track and print the best parameter combination and its mean AUC.\n",
    "\n",
    "\n",
    "        # Create the final pipeline for this classifier\n",
    "        print(\"-\" * 90)\n",
    "        print(f\"Best parameters for {name}: {best_params}\")\n",
    "        print(f\"Best cross-validation AUC: {best_score:.4f}\")\n",
    "\n",
    "        final_pipeline = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('feature_selection', SelectKBest(f_classif)),\n",
    "            ('classifier', classifier_instance)\n",
    "        ])\n",
    "\n",
    "        # Set the best parameters found\n",
    "        final_pipeline.set_params(**best_params)\n",
    "\n",
    "        # Fit the final pipeline on the full training data\n",
    "        final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Store the fully trained best pipeline\n",
    "        best_estimators[name] = final_pipeline\n",
    "\n",
    "    return best_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRUAXSXaXUJG"
   },
   "source": [
    "**Understanding the Manual Implementation**:\n",
    "- **Nested Cross-Validation**: For each parameter combination, we perform 5-fold CV\n",
    "- **Pipeline Integration**: Each step (scaling, feature selection, classification) is properly chained\n",
    "- **AUC Scoring**: We use Area Under the ROC Curve as our optimization metric\n",
    "- **Best Model Selection**: The combination with highest mean AUC across folds is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-4A_DGpXotU"
   },
   "source": [
    "## Part 5: Built-in Grid Search Implementation\n",
    "\n",
    "Now let's compare our manual implementation with scikit-learn's GridSearchCV.\n",
    "\n",
    "\n",
    "\n",
    "**Advantages of Built-in GridSearchCV**:\n",
    "- **Parallel Processing**: Uses `n_jobs=-1` for faster computation\n",
    "- **Cleaner Code**: Less verbose than manual implementation\n",
    "- **Built-in Features**: Automatic best model selection and scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edhV1LPEaOdb"
   },
   "outputs": [],
   "source": [
    "def run_builtin_grid_search(X_train, y_train, dataset_name):\n",
    "    \"\"\"Run built-in grid search and return best estimators\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING BUILT-IN GRID SEARCH FOR {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    results_builtin = {}\n",
    "\n",
    "    # Adjust parameter grids based on dataset size\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    for classifier_instance, param_grid, name in classifiers_to_tune:\n",
    "        print(f\"\\n--- GridSearchCV for {name} ---\")\n",
    "\n",
    "        # TODO: Implement built-in grid search for each classifier:\n",
    "        # - Adjust feature selection parameter grid based on dataset size (n_features)\n",
    "        # - Create a pipeline with StandardScaler, SelectKBest(f_classif), and the classifier\n",
    "        # - Set up StratifiedKFold cross-validation\n",
    "        # - Run GridSearchCV with the pipeline and adjusted param grid\n",
    "        # - Fit grid search on training data and collect best estimator/results\n",
    "\n",
    "        # Example code fragments that may be needed:\n",
    "        # n_features = X_train.shape[1]\n",
    "        # pipeline = Pipeline(steps=[\n",
    "        #     ('scaler', StandardScaler()),\n",
    "        #     ('feature_selection', SelectKBest(f_classif)),\n",
    "        #     ('classifier', classifier_instance)\n",
    "        # ])\n",
    "        # cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Save results\n",
    "        results_builtin[name] = {\n",
    "            'best_estimator': grid_search.best_estimator_,\n",
    "            'best_score (CV)': grid_search.best_score_,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        print(f\"Best params for {name}: {results_builtin[name]['best_params']}\")\n",
    "        print(f\"Best CV score: {results_builtin[name]['best_score (CV)']:.4f}\")\n",
    "\n",
    "    return results_builtin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk7sL-SRXlow"
   },
   "source": [
    "## Part 6: Model Evaluation and Voting Classifiers\n",
    "\n",
    "This function evaluates individual models and creates voting classifiers to combine their predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLlH3BDoabcE"
   },
   "outputs": [],
   "source": [
    "def evaluate_models(X_test, y_test, best_estimators, dataset_name, method_name=\"Manual\"):\n",
    "    \"\"\"Evaluate models and create visualizations\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING {method_name.upper()} MODELS FOR {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Individual model evaluation\n",
    "    print(f\"\\n--- Individual Model Performance ---\")\n",
    "    for name, model in best_estimators.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(f\"  Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"  Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"  F1-Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "        print(f\"  ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "    # Voting Classifier\n",
    "    print(f\"\\n--- {method_name} Voting Classifier ---\")\n",
    "\n",
    "    if method_name == \"Manual\":\n",
    "        # Manual voting implementation\n",
    "        y_pred_votes = []\n",
    "        y_pred_proba_avg = []\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            votes = []\n",
    "            probas = []\n",
    "\n",
    "            for name, model in best_estimators.items():\n",
    "                pred = model.predict(X_test.iloc[[i]])[0]\n",
    "                proba = model.predict_proba(X_test.iloc[[i]])[0, 1]\n",
    "                votes.append(pred)\n",
    "                probas.append(proba)\n",
    "\n",
    "            majority_vote = 1 if np.mean(votes) > 0.5 else 0\n",
    "            avg_proba = np.mean(probas)\n",
    "\n",
    "            y_pred_votes.append(majority_vote)\n",
    "            y_pred_proba_avg.append(avg_proba)\n",
    "\n",
    "        y_pred_votes = np.array(y_pred_votes)\n",
    "        y_pred_proba_avg = np.array(y_pred_proba_avg)\n",
    "\n",
    "    else:  # Built-in\n",
    "        # Create VotingClassifier\n",
    "        estimators = [(name, model) for name, model in best_estimators.items()]\n",
    "        voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "        voting_clf.fit(X_train, y_train)  # Note: This assumes X_train, y_train are in scope\n",
    "\n",
    "        y_pred_votes = voting_clf.predict(X_test)\n",
    "        y_pred_proba_avg = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute voting metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_votes)\n",
    "    precision = precision_score(y_test, y_pred_votes, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_votes, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_votes, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba_avg)\n",
    "\n",
    "    print(f\"Voting Classifier Performance:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}, Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # Visualizations\n",
    "    # ROC Curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, model in best_estimators.items():\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "    # Add voting classifier to ROC\n",
    "    fpr_vote, tpr_vote, _ = roc_curve(y_test, y_pred_proba_avg)\n",
    "    plt.plot(fpr_vote, tpr_vote, label=f'Voting (AUC = {auc:.3f})', linewidth=3, linestyle='--')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {dataset_name} ({method_name})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Confusion Matrix for Voting Classifier\n",
    "    plt.subplot(1, 2, 2)\n",
    "    cm = confusion_matrix(y_test, y_pred_votes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=plt.gca(), cmap=\"Blues\")\n",
    "    plt.title(f'Voting Classifier - {dataset_name} ({method_name})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred_votes, y_pred_proba_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwzQEU5pZMDI"
   },
   "source": [
    "## Part 7: Complete Pipeline Function\n",
    "\n",
    "This function orchestrates the entire experiment for each dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGPXOfWaaf_n"
   },
   "outputs": [],
   "source": [
    "def run_complete_pipeline(dataset_loader, dataset_name):\n",
    "    \"\"\"Run complete pipeline for a dataset\"\"\"\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"PROCESSING DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, X_test, y_train, y_test, actual_name = dataset_loader()\n",
    "    if X_train is None:\n",
    "        print(f\"Skipping {dataset_name} due to loading error.\")\n",
    "        return\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Part 1: Manual Implementation\n",
    "    manual_estimators = run_manual_grid_search(X_train, y_train, actual_name)\n",
    "    manual_votes, manual_proba = evaluate_models(X_test, y_test, manual_estimators, actual_name, \"Manual\")\n",
    "\n",
    "    # Part 2: Built-in Implementation\n",
    "    builtin_results = run_builtin_grid_search(X_train, y_train, actual_name)\n",
    "    builtin_estimators = {name: results['best_estimator']\n",
    "                         for name, results in builtin_results.items()}\n",
    "    builtin_votes, builtin_proba = evaluate_models(X_test, y_test, builtin_estimators, actual_name, \"Built-in\")\n",
    "\n",
    "    print(f\"\\nCompleted processing for {actual_name}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luYsJ4GtZbz7"
   },
   "source": [
    "## Part 8: Execute the Complete Lab\n",
    "\n",
    "Now let's run our pipeline on all four datasets!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwgQCoI1ZcSa"
   },
   "outputs": [],
   "source": [
    "# --- Run Pipeline for All Datasets ---\n",
    "datasets = [\n",
    "    (load_wine_quality, \"Wine Quality\"),\n",
    "    (load_hr_attrition, \"HR Attrition\"),\n",
    "    (load_banknote, \"Banknote Authentication\"),\n",
    "    (load_qsar_biodegradation, \"QSAR Biodegradation\")\n",
    "]\n",
    "\n",
    "# Run for each dataset\n",
    "for dataset_loader, dataset_name in datasets:\n",
    "    try:\n",
    "        run_complete_pipeline(dataset_loader, dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL DATASETS PROCESSED!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
